
\subsection*{Results}
The results in Tables \ref{results} and \ref{tab:ResultsonDev-dataset} suggest that the models performed well on the "O" class but had lower performance on the "B-neg" and "I-neg" classes. For both models, the precision, recall, and F1-score values were close to 1 for the "O" class, indicating that the models made very few false positive and false negative predictions. However, the precision, recall, and F1-score values were lower for the "B-neg" and "I-neg" classes, suggesting that the models had a higher number of false positive and false negative predictions for these classes. Furthermore, As shown in \ref{fig:feature importance}, the most important features for XGBoost were "tag" (POS tag) and "head" (sentence root). Unfortunately, since Gridsearch suggested that $kernel$ should be set to $polynomial$ for the SVM models, it was not possible to extract information about feature importance for them.  

 
\begin{table}[!h]
{\fontsize{9}{4}
\textit{Note 1}: Both models produce the same results for both testing datasets.
\\
\textit{Note 2}: There are fewer observations because we dropped missing values.}
\centering
\begin{tabular}{cc|cccc|cccc}
\hline
 &  &  &  & XGBoost &  &  &  & SVM &  \\ \hline
Technique & Class & Precision & Recall & f1-score & Support & Precision & Recall & f1-score & Support \\ \hline
 & O & 1.00 & 1.00 & 1.00 & 8151 & 1.00 & 1.00 & 1.00 & 8151 \\
No Oversampling & B-neg & 0.89 & 0.89 & 0.89 & 129 & 0.90 & 0.89 & 0.89 & 129 \\
 & I-neg & 0.00 & 0.00 & 0.00 & 4 & 0.00 & 0.00 & 0.00 & 4 \\
Macro-avg &  & 0.63 & 0.63 & 0.63 & 8284 & 0.63 & 0.63 & 0.63 & 8284 \\ \hline
 & O & 1.00 & 0.97 & 0.98 & 8151 & 1.00 & 0.98 & 0.99 & 8151 \\
Oversampling & B-neg & 0.88 & 0.77 & 0.82 & 129 & 0.88 & 0.71 & 0.78 & 129 \\
 & I-neg & 0.01 & 0.50 & 0.02 & 4 & 0.00 & 0.00 & 0.00 & 4 \\
Macro-avg &  & 0.63 & 0.75 & 0.61 & 8284 & 0.63 & 0.56 & 0.59 & 8284 \\ \hline
\end{tabular}

\smallskip
\caption{Experiment Results on Tests Datasets}
%\smallskip
\label{results}
\end{table}
 
The use of oversampling had a limited impact on the overall performance of the models, with the macro-average precision, recall, and F1-score values being slightly higher for the oversampled data compared to the non-oversampled data. However, it is important to note that oversampling can have limitations, such as potentially causing overfitting and reduced interpretability of the model. 

\begin{table}[!h]
{\fontsize{9}{4}
\textit{Note}: There are fewer observations because we dropped missing values.}
\centering
\begin{tabular}{cc|cccc|cccc}
\hline
 &  &  &  & XGBoost &  &  &  & SVM &  \\ \hline
Technique & Class & Precision & Recall & f1-score & Support & Precision & Recall & f1-score & Support \\ \hline
 & O & 1.00 & 1.00 & 1.00 & 12375 & 1.00 & 1.00 & 1.00 & 12375 \\
No Oversampling & B-neg & 0.84 & 0.80 & 0.82 & 168 & 0.93 & 0.79 & 0.85 & 168 \\
 & I-neg & 0.00 & 0.00 & 0.00 & 3 & 0.00 & 0.00 & 0.00 & 3 \\
Macro-avg &  & 0.61 & 0.60 & 0.61 & 12546 & 0.64 & 0.59 & 0.62 & 12546 \\ \hline
 & O & 1.00 & 0.98 & 0.99 & 12375 & 1.00 & 0.98 & 0.99 & 12375 \\
Oversampling & B-neg & 0.91 & 0.68 & 0.78 & 168 & 0.91 & 0.64 & 0.75 & 168 \\
 & I-neg & 0.00 & 0.00 & 0.00 & 3 & 0.00 & 0.00 & 0.00 & 3 \\
Macro-avg &  & 0.63 & 0.56 & 0.59 & 12546 & 0.64 & 0.54 & 0.58 & 12546 \\ \hline
\end{tabular}
\caption{Experiment Results on Dev-Dataset}
\label{tab:ResultsonDev-dataset}
\end{table}

Moreover, Figure \ref{fig:confusion_matrix} shows the confusion matrices of these 4 models on the tests sets. For XGBoost with oversampling, the model correctly classified 7965 samples as positive (O), 99 samples as B-neg, and 2 samples as I-neg. However, it misclassified 174 samples as B-neg, 1 sample as I-neg, and 12 samples as O. For SVM with oversampling, the model correctly classified 7984 samples as O, 91 samples as B-neg, and 0 samples as I-neg. However, it misclassified 156 samples as B-neg, 9 samples as I-neg, and 11 samples as O. For XGBoost without oversampling, the model correctly classified 8138 samples as O, 115 samples as B-neg, and 0 samples as I-neg. However, it misclassified 13 samples as B-neg, and 3 samples as I-neg. For SVM without oversampling, the model correctly classified 8139 samples as O, 115 samples as B-neg, and 0 samples as I-neg. However, it misclassified 12 samples as B-neg, and 3 samples as I-neg. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{images/confusion_matrices.pdf}
    \caption{Confusion Matrices}
    \label{fig:confusion_matrix}
\end{figure}

In general, The results indicate that the XGBoost model with oversampling performed better than the SVM model with oversampling, with higher precision and recall scores for the positive class and the B-neg class. However, both models without oversampling performed similarly and had better performance for the positive class but lower performance for the B-neg and I-neg class. However, further improvement is required for the "B-neg" and "I-neg" classes. Further experimentation on more balanced datasets is necessary to determine if an improvement in performance for these classes can be achieved.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\textwidth]{images/feature importance.png}
    \caption{Feature Importance}
    \label{fig:feature importance}
\end{figure}


\subsection*{Error Analysis}

In order to get a better understanding of the capabilities and weaknesses of our models, qualitative error analysis of the predictions made by the best model (XGBoost with oversampling) over the SEM 2012 circle test set was performed.

Hossain et. al. \cite{hossain2020predicting} highlights the importance of performing this type of analysis when building a negation classifier, as negation is a complex semantic phenomenon that interacts with other aspects of the meaning and structure of sentences, and this complexity is reflected in the diversity of errors. Cruz et. al. \cite{cruz2016machine} also performed a qualitative error analysis focusing on the false negative (FN) and false positive (FP) errors. A false negative error occurs when the system does not identify a negative cue token that is marked as such in the set. A false positive error occurs when the classifier identifies a negative cue token that is not marked as such in the set.

There is a total of 134 false positives divided into 12 where the model classified the token with the class B-NEG and 122 with I-NEG. From the 12 B-NEG golden label tokens, 8 of them correspond to the token \textit{not}. Some of these cases are shown in Table \ref{tab:fps}. It is very interesting to note that all the 134 false positives do not contain an affix from the list described in Section \ref{sec:featureextraction} (\textit{hasNegAffix} feature), and in the case of the 122 I-NEG classified tokens, all of them are not present in the list of highly probable negation expressions (\textit{NegExpList} feature), while all the 12 B-NEG classified tokens are.


\begin{table}
\centering
\refstepcounter{table}
\begin{tabular}{lccc}
\multicolumn{1}{c}{Sentence}                                                                                                                                                                                                                              & Token   & Golden Label & Prediction  \\ 
\hline
Why not write?                                                                                                                                                                                                                                           & not     & O            & B-NEG       \\
Suggestive, Watson, is it not?                                                                                                                                                                                                                         & not     & O            & B-NEG       \\
\begin{tabular}[c]{@{}l@{}}How is any news or any message to reach\\him from without?\end{tabular}                                                                                                                                                        & without & O            & B-NEG       \\
\begin{tabular}[c]{@{}l@{}}It 's all very appropriate to Mrs. Warren 's \\lodger.\end{tabular}                                                                                                                                                            & all     & O            & I-NEG       \\
That is definite enough.                                                                                                                                                                                                                                  & enough  & O            & I-NEG       \\
\begin{tabular}[c]{@{}l@{}}Finally	Gennaro	told	me,	through	the	paper,\\that		he		would		signal		to		me		from~a		certain	\\window,		but		when		the		signals		came		they	\\were		nothing		but		warnings,~which		were	\\suddenly		interrupted.\end{tabular} & nothing & O            & B-NEG       \\
\hline
\end{tabular}
\caption{\label{tab:fps}Sample of false positives in test set predictions}
\end{table}


Also, there is a total of 31 false negatives divided into 29 where the model predicted O and the golden label is B-NEG, and only 2 where the golden label is I-NEG and the model predicted O. From these 29 errors, 16 corresponds to the token \textit{n't}, which \cite{cruz2016machine} defines as incorrect classification of a \textit{multiword cue}. A sample of these errors is shown in Table \ref{tab:fns}. It is interesting to note that in both errors where the golden label is I-NEG, the token associated is \textit{more} but in different contexts and making a different usage of the word. In both cases, the token is neither in the \textit{NegExpList} list nor containing an affix from the affixes list (\textit{hasNegAffix} feature).

\begin{table}
\centering
\refstepcounter{table}
\begin{tabular}{lccc}
\multicolumn{1}{c}{Sentence}                                                                                                           & Token   & Golden Label & Prediction  \\ 
\hline
You don't object to tobacco, I take it?                                                                                                & n't     & B-NEG        & O           \\
I can't sleep for fright .                                                                                                             & n't     & B-NEG        & O           \\
I 'll have no more of it!                                                                                                              & more    & I-NEG        & O           \\
\begin{tabular}[c]{@{}l@{}}He struck Gennaro senseless and fled from\\the house which he was never more to enter.\end{tabular}         & more    & I-NEG        & O           \\
\begin{tabular}[c]{@{}l@{}}But surely the most valuable hunting-ground\\that ever was given to a student of the\\unusual!\end{tabular} & unusual & B-NEG        & O           \\
At first, I thought that it was dislike.                                                                                                & dislike & B-NEG        & O           \\
\hline
\end{tabular}
\caption{\label{tab:fns}Sample of false negatives in test set predictions}
\end{table}