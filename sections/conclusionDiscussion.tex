In general, as was described in the Evaluation section, all four models did not seem to differ significantly in terms of performance for both datasets. Therefore, it can be concluded that using Random OverSampling does not improve the negation-detecting performance. This could indicate, that the problem does not solely lie in the imbalance of the dataset itself, and rather points out that neither SVM nor XGBoost is capable of detecting the negation pattern completely. While both models seem to perform quite well detecting non-negation, their performance decreases significantly on detecting negation. 

One explanation of such behaviour could be that in this research n-grams of words were not added to the list of features, which, coming back to  Lapponi et. al.\cite{lapponi2012uio}, seems to be the only difference, except for adding dependency features. This could also explain the poor performance of the XGBoost model. Especially for the "I-NEG" tag, the n-grams for each token seem to be incremental to detect the inside negation cue. On the other hand, Lapponi et. al. do not present the results of the model for each class separately, rather, they show micro-averaged precision, recall and f1-scores, so it is not clear, whether they experienced the drop of performance detecting negation cues as opposed to non-negation. Nevertheless, their micro-averaged scores are still considerably higher, than those, found in this paper.  

Another explanation for such behaviour still seems to concern the dataset itself. Lapponi et.al. had the full SEM-dataset, while for this research a shortened version was used, which could have influenced, how much learning each model was allowed to do. So for further research, it is important to try the same models on the full dataset to see, if it increases the performance. Moreover, it seems like in general more balanced datasets are needed for such tasks. In the preliminary experiments, not reported anywhere above, the problems were encountered with the imbalance of the datasets, where the models would just classify all tokens to be non-negation, and be correct most of the time. 

Last but not least, it is important to note, that, as pointed out in the error analysis section, the XGBoost model seemed to detect a lot of false negation cues, such as $nothing$ in fixed expression $nothing, but ...$. For future research it might be important to consider compiling a separate dictionary of expressions, containing words from \textit{NegExpList}. 
