A qualitative error analysis was conducted on the predictions made by both the XGBoost and SVM models to gain insight into their capabilities and weaknesses when applied to the SEM 2012 circle test set. Performing such an analysis is crucial when building a negation classifier due to the complex semantic nature of negation, which interacts with various aspects of meaning and sentence structure \cite{hossain2020predicting, cruz2016machine}.

\begin{table}[!h]
\caption{\label{tab:fns}Sample of false negatives in test set predictions}
\centering
\begin{tabular}{ccccc}
\hline
                                        Sentence &    Token & Golden Label & \begin{tabular}[c]{@{}l@{}}Prediction\\ (SVM)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Prediction\\ (XGBoost)\end{tabular} \\
\hline
                  `` I do n't understand that . &      n't &  B-NEG &              O &              O \\
       You do n't object to tobacco , I take it ? &      n't &  B-NEG &              O &              O \\
                      I ca n't sleep for fright . &      n't &  B-NEG &              O &              O \\
                     `` I 'll have no more of it ! &     more &  I-NEG &              O &              O \\
                    That ca n't be all , Watson ? &      n't &  B-NEG &              O &              O \\
 ` If not , I 'll have no more to do with you . ' &     more &  I-NEG &              O &              O \\
         At first I thought that it was dislike . &  dislike &  B-NEG &              O &              O \\
\hline
\end{tabular}
\end{table}



\\

Examining the false positive (FP) and false negative (FN) errors is particularly informative. An FP error occurs when the classifier identifies a negative cue token that is not annotated as such in the dataset, while an FN error takes place when the system fails to identify a token annotated as a negative cue.





\begin{table}[!h]
    \centering
    \caption{\label{tab:fps}Sample of false positives in test set predictions}
\begin{tabular}{ccccc}
\hline
                                          Sentence &      Token & Golden Label & \begin{tabular}[c]{@{}l@{}}Prediction\\ (SVM)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Prediction\\ (XGBoost)\end{tabular} \\
\hline
           It sounds plausible , does it not ? &        not &      O &          B-NEG &          B-NEG \\
       Yes , here we are -- three days later . &       days &      O &          B-NEG &          B-NEG \\
      `` No doubt , sir ; but this is different . &         No &      O &          B-NEG &          B-NEG \\
      `` No doubt , sir ; but this is different . &          . &      O &          B-NEG &          B-NEG \\
         Then comes something much more definite : &  something &      O &          B-NEG &          B-NEG \\
  `` That was bad enough , but worse was to come . &       come &      O &          I-NEG &          I-NEG \\
                       One A , two B , and so on . &        and &      O &          B-NEG &          B-NEG \\
                    One A , two B , and so on . &         so &      O &          B-NEG &          B-NEG \\
                             You will hear soon . &        You &      O &          B-NEG &          B-NEG \\
                             You will hear soon . &       hear &      O &          B-NEG &          B-NEG \\
                                Is it not so ? '' &         '' &      O &          B-NEG &          B-NEG \\
              Your presence here was desirable . &          . &      O &          B-NEG &          B-NEG \\
 It was fear -- a deep , secret , shrinking fear . &     secret &      O &          B-NEG &          B-NEG \\

\hline
\end{tabular}
\end{table}

\\

For XGBoost, the FP errors include 341 instances of B-NEG and 16 instances of I-NEG when the golden label is O. In comparison, SVM has 208 FP errors for B-NEG and 17 for I-NEG with the golden label as O. On the other hand, FN errors for XGBoost consist of 38 instances where the prediction is O, and the golden label is B-NEG, and 3 instances where the golden label is I-NEG. SVM presents a similar distribution with 29 FN errors for B-NEG and 3 for I-NEG when the model prediction is O.




Some of these errors stem from the incorrect classification of a multiword cue, such as with the token "n't" \cite{cruz2016machine}, which may occur in both FP and FN errors. It is worthy of note that all FP errors lack an affix from the list discussed in Section \ref{sec:featEx} ("hasNegAffix" feature) and are not part of the "NegExpList" feature.
\\

To improve future classifications, it is essential to consider the complexity of negation, the importance of features such as 'hasNegAffix' and 'NegExpList,' and the context in which tokens appear. By refining the training data and further exploring the relationship between these features, classifiers may yield higher precision and recall scores, ultimately improving negation detection.